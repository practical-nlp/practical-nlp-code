{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Pre_Trained_Word_Embeddings.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z89NDA-ToPnT",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://)In this notebook, let us see how we can represent text using pre-trained word embedding models, as well as train our own word and document embedding models.\n",
        "\n",
        "# 1. Using a pre-trained word2vec model\n",
        "\n",
        "Let us take an example of a pre-trained word2vec model, and how we can use it to look for most similar words. We will use the Google News vectors.\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "\n",
        "A few other pre-trained word embedding models, and details on the means to access them through gensim can be found in:\n",
        "https://github.com/RaRe-Technologies/gensim-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTpzLd6dvB6Q",
        "colab_type": "code",
        "outputId": "55089da0-4d12-4511-a1d1-033d321af9e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-12 04:55:21--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.142.54\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.142.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  68.9MB/s    in 28s     \n",
            "\n",
            "2020-03-12 04:55:54 (55.9 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBsTuJ5FwAFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import os\n",
        "import psutil\n",
        "process = psutil.Process(os.getpid())\n",
        "\n",
        "from psutil import virtual_memory\n",
        "mem = virtual_memory()\n",
        "\n",
        "import time\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aodBmqZToPnY",
        "colab_type": "code",
        "outputId": "e48f3d0d-7425-4874-cedd-abd122c19030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "pretrainedpath = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "#Load W2V model. This will take some time, but it is a one time effort! \n",
        "pre = process.memory_info().rss\n",
        "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9)))\n",
        "print('-'*10)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True)\n",
        "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time))\n",
        "print('-'*10)\n",
        "\n",
        "print('Finished loading Word2Vec')\n",
        "print('-'*10)\n",
        "\n",
        "post = process.memory_info().rss\n",
        "print(\"Memory used in GB after Loading the Model: %0.2f\"%float(post/(10**9)))\n",
        "print('-'*10)\n",
        "\n",
        "ttl = mem.total\n",
        "print(\"Percentage increase in memory usage: %0.2f \"%float((post/pre)*100))\n",
        "print('-'*10)\n",
        "\n",
        "print(\"Numver of words in vocablulary: \",len(w2v_model.vocab)) #Number of words in the vocabulary. "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory used in GB before Loading the Model: 0.19\n",
            "----------\n",
            "107.37 seconds taken to load\n",
            "----------\n",
            "Finished loading Word2Vec\n",
            "----------\n",
            "Memory used in GB after Loading the Model: 5.04\n",
            "----------\n",
            "Percentage increase in memory usage: 2617.63 \n",
            "----------\n",
            "Numver of words in vocablulary:  3000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-SVDF3Av-2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhJ_488PoPnr",
        "colab_type": "code",
        "outputId": "fdb39b81-a444-4394-d6fc-a15cd42ddc49",
        "colab": {}
      },
      "source": [
        "#Let us examine the model by knowing what the most similar words are, for a given word!\n",
        "w2v_model.most_similar('beautiful')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gorgeous', 0.8353004455566406),\n",
              " ('lovely', 0.810693621635437),\n",
              " ('stunningly_beautiful', 0.7329413890838623),\n",
              " ('breathtakingly_beautiful', 0.7231341004371643),\n",
              " ('wonderful', 0.6854087114334106),\n",
              " ('fabulous', 0.6700063943862915),\n",
              " ('loveliest', 0.6612576246261597),\n",
              " ('prettiest', 0.6595001816749573),\n",
              " ('beatiful', 0.6593326330184937),\n",
              " ('magnificent', 0.6591402292251587)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1Or5oG5oPn1",
        "colab_type": "code",
        "outputId": "061afff1-6277-4111-e363-4bf371964356",
        "colab": {}
      },
      "source": [
        "#Let us try with another word! \n",
        "w2v_model.most_similar('toronto')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('montreal', 0.698411226272583),\n",
              " ('vancouver', 0.6587257385253906),\n",
              " ('nyc', 0.6248831748962402),\n",
              " ('alberta', 0.6179691553115845),\n",
              " ('boston', 0.611499547958374),\n",
              " ('calgary', 0.61032634973526),\n",
              " ('edmonton', 0.6100261211395264),\n",
              " ('canadian', 0.5944076776504517),\n",
              " ('chicago', 0.5911980271339417),\n",
              " ('springfield', 0.5888351202011108)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtQiYOR9oPn_",
        "colab_type": "code",
        "outputId": "915021a6-f610-4f01-9402-912c160db089",
        "colab": {}
      },
      "source": [
        "#What is the vector representation for a word? \n",
        "w2v_model['computer']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
              "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
              "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
              "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
              "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
              "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
              "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
              "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
              "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
              "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
              "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
              "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
              "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
              "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
              "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
              "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
              "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
              "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
              "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
              "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
              "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
              "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
              "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
              "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
              "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
              "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
              "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
              "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
              "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
              "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
              "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
              "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
              "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
              "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
              "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
              "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
              "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
              "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
              "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
              "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
              "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
              "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
              "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
              "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
              "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
              "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
              "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
              "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
              "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
              "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
              "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
              "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
              "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
              "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
              "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
              "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
              "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
              "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
              "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
              "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
              "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
              "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
              "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
              "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
              "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
              "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
              "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
              "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
              "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
              "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
              "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
              "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
              "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
              "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
              "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoeH_gfroPoJ",
        "colab_type": "code",
        "outputId": "7bee1f98-9f47-4326-cfc9-626a7f14ca41",
        "colab": {}
      },
      "source": [
        "#What if I am looking for a word that is not in this vocabulary?\n",
        "w2v_model['practicalnlp']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"word 'practicalnlp' not in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-354849ef77a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#What if I am looking for a word that is not in this vocabulary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'practicalnlp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'practicalnlp' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "douwnKS5oPoS",
        "colab_type": "text"
      },
      "source": [
        "####Two things to note while using pre-trained models: \n",
        "\n",
        "\n",
        "1.   Tokens/Words are always lowercased. If a word is not in the vocabulary,   the model throws an exception.\n",
        "2.   So, it is always a good idea to encapsulate those statements in try/except blocks.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjWxh9kBoPob",
        "colab_type": "text"
      },
      "source": [
        "# 2. Getting the embedding representation for full text\n",
        "\n",
        "We have seen how to get embedding vectors for single words. How do we use them to get such a representation for a full text? A simple way is to just sum or average the embeddings for individual words. We will see an example of this using Word2Vec in Chapter 4. Let us see a small example using another NLP library Spacy - which we saw earlier in Chapter 2 too.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFLuSb9ZoPoc",
        "colab_type": "code",
        "outputId": "35fe37df-f1bd-4c64-c8ad-0455980966ee",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spacy model that we already installed in Chapter 2. This takes a few seconds.\n",
        "%time nlp = spacy.load('en_core_web_md')\n",
        "# process a sentence using the model\n",
        "mydoc = nlp(\"Canada is a large country\")\n",
        "#Get a vector for individual words\n",
        "#print(doc[0].vector) #vector for 'Canada', the first word in the text \n",
        "print(doc.vector) #Averaged vector for the entire sentence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 13 s, sys: 661 ms, total: 13.7 s\n",
            "Wall time: 15.2 s\n",
            "[-1.12055197e-01  2.26087615e-01 -5.15111461e-02 -1.21812008e-01\n",
            "  4.13958639e-01 -8.56475979e-02 -2.84600933e-03 -2.26096585e-01\n",
            "  6.98113963e-02  2.27946019e+00 -4.49774921e-01 -6.39050007e-02\n",
            " -1.80326015e-01 -8.79765972e-02  9.93399299e-04 -1.57384202e-01\n",
            " -1.23817801e-01  1.54990411e+00  2.00794004e-02  1.38399601e-01\n",
            " -1.48897991e-01 -2.23025799e-01 -1.48171991e-01  4.68924567e-02\n",
            " -3.17026004e-02  1.19096041e-02 -6.10985979e-02  9.57068056e-02\n",
            "  9.37099904e-02  1.70955807e-01 -9.29740071e-03  7.88536817e-02\n",
            "  1.74508005e-01 -1.04450598e-01  1.04872189e-01 -1.16961405e-01\n",
            "  6.23028055e-02 -2.23016590e-01 -1.44107476e-01 -2.03423887e-01\n",
            "  2.61404991e-01  2.43404001e-01  1.51980996e-01 -1.12484001e-01\n",
            "  1.18055798e-01 -9.51323956e-02  8.66319984e-02 -2.54322797e-01\n",
            "  3.84932049e-02  1.18278004e-01 -3.21602583e-01  3.73764008e-01\n",
            "  1.13018408e-01 -8.05834010e-02  1.84921592e-01  9.38879885e-03\n",
            "  1.22166201e-01 -3.24288011e-02  1.01590000e-01 -1.56877995e-01\n",
            " -2.57006437e-02  1.63392588e-01  1.06118001e-01  2.25193188e-01\n",
            "  8.06204006e-02 -1.21081993e-01 -1.52107209e-01  8.25726017e-02\n",
            " -6.09899946e-02  1.44145802e-01  2.01554038e-02  2.54258011e-02\n",
            "  1.06071997e-02  6.37948066e-02  1.10551611e-01 -6.40176088e-02\n",
            " -6.36451989e-02 -9.99798030e-02 -7.01020136e-02  3.09334368e-01\n",
            "  5.68300001e-02  3.63879651e-03 -1.65255398e-01  2.98442870e-01\n",
            "  4.01660334e-03 -1.73631594e-01  5.15965708e-02  1.61811799e-01\n",
            "  2.20304996e-01 -8.29614028e-02 -2.64678001e-01  2.44114012e-01\n",
            "  3.48895532e-03 -1.57521993e-01  1.67974800e-01  1.05541132e-01\n",
            " -1.31224409e-01  7.17941970e-02  1.39708191e-01 -1.95359858e-03\n",
            " -8.55428055e-02  1.20119795e-01 -6.84404075e-02  5.14601183e-04\n",
            " -2.86250003e-02 -1.10662603e+00  2.02491835e-01 -1.50410801e-01\n",
            "  6.51507173e-03 -3.30360234e-03  1.21523812e-01 -1.61614027e-02\n",
            " -1.43233404e-01 -9.88139957e-02 -2.17486005e-02  1.81988999e-01\n",
            "  8.85506049e-02  2.72242010e-01 -7.73219988e-02  1.43622067e-02\n",
            " -1.57062009e-01  4.01146002e-02  3.90305184e-02 -1.42812401e-01\n",
            " -2.08329991e-01  9.64459926e-02  1.42821997e-01 -1.94155991e-01\n",
            "  5.37982993e-02 -1.00471973e-02  1.94714032e-02 -9.83514041e-02\n",
            " -4.17162031e-02  1.23069003e-01  1.68428212e-01 -1.17991492e-01\n",
            " -2.56704390e-01 -1.89464003e-01  9.22677964e-02 -1.72503412e-01\n",
            " -1.11929202e+00  6.42500073e-03  3.51435989e-01  8.19059983e-02\n",
            "  4.92408946e-02 -1.80243999e-01  1.82863399e-01  8.92240033e-02\n",
            "  2.47399211e-01  2.74492018e-02 -2.49322020e-02  2.35055804e-01\n",
            "  8.12319964e-02 -1.86482631e-02 -1.06439434e-01  5.28851971e-02\n",
            " -1.02569997e-01  1.35777995e-01 -2.32603997e-01  9.24602076e-02\n",
            "  1.92440599e-01  1.48551196e-01  5.57186007e-02  3.97088043e-02\n",
            " -6.74048066e-03  9.73687991e-02  2.62231939e-02 -8.26509967e-02\n",
            "  1.30085424e-01 -1.38572007e-01 -4.11808006e-02 -4.13070023e-02\n",
            " -3.41880023e-02  1.28202796e-01 -6.66912049e-02 -7.41944537e-02\n",
            " -5.87003939e-02  1.36300415e-01  1.67494014e-01  1.71119809e-01\n",
            "  1.18692197e-01  2.30142009e-02 -2.06086040e-02 -3.85930002e-01\n",
            " -1.17673976e-02 -7.34595209e-02 -3.43096368e-02 -7.80718103e-02\n",
            " -2.81003956e-02 -7.30765983e-02 -2.21649408e-01 -1.02057599e-01\n",
            "  5.11020012e-02 -9.07440037e-02 -4.69896048e-02 -2.10200553e-03\n",
            "  1.05816983e-01  4.79107983e-02  1.03080198e-01 -8.96641985e-02\n",
            "  8.85651931e-02 -9.09178331e-02 -5.16167991e-02  1.50742605e-01\n",
            "  3.07500213e-01 -4.05239780e-03  1.04269005e-01  3.55780013e-02\n",
            "  1.16165996e-01 -2.97939777e-03 -1.42966792e-01  5.00957891e-02\n",
            " -1.08308598e-01  1.68199837e-03  1.36314392e-01  1.48694202e-01\n",
            " -3.17817986e-01  1.21000603e-01 -1.59556001e-01  7.51644000e-02\n",
            " -1.03386201e-01  1.10754207e-01  8.20529982e-02 -6.02059904e-03\n",
            "  1.35578603e-01 -4.08943966e-02  6.05328009e-02  1.03734590e-01\n",
            " -6.22724071e-02  2.30276197e-01  1.30762011e-01  1.51950002e-01\n",
            "  7.40183964e-02 -1.84507206e-01 -1.33174613e-01 -1.49338007e-01\n",
            "  1.19309977e-01 -2.41554022e-01 -1.00904807e-01  1.54562384e-01\n",
            " -7.63845369e-02  1.66379198e-01  2.20374197e-01  1.58361979e-02\n",
            "  1.80677801e-01 -1.77342609e-01  2.22857997e-01 -2.99477577e-01\n",
            " -1.53620601e-01 -2.67919600e-01  1.56353399e-01 -1.74718007e-01\n",
            "  1.83644608e-01  1.28259212e-01 -6.30084053e-02  2.68236816e-01\n",
            "  2.10368007e-01 -4.73994762e-02 -1.09680817e-01  1.62620202e-01\n",
            "  8.96113962e-02  1.50361210e-01 -1.55037967e-02  1.50141995e-02\n",
            "  1.76618043e-02 -2.28057191e-01  7.85290003e-02 -4.59632799e-02\n",
            "  1.98103897e-02 -1.71379801e-02 -1.45676598e-01 -3.32076550e-02\n",
            " -2.09102005e-01 -2.48584002e-01 -8.51256028e-02  4.25900035e-02\n",
            " -1.33966401e-01  2.89979968e-02  2.10713193e-01 -1.86206046e-02\n",
            "  1.71603993e-01  2.21868396e-01 -2.10479975e-01  1.49794608e-01\n",
            " -1.10692397e-01 -4.47340589e-03  5.13652042e-02 -7.27116019e-02\n",
            "  6.07413948e-02 -8.13369974e-02 -1.94639400e-01 -5.06809242e-02\n",
            "  6.40980080e-02 -2.20814198e-01  2.96969917e-02  1.53438210e-01\n",
            " -2.18270030e-02 -1.93358198e-01 -2.26220220e-01  1.84093148e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-8wrQlLoPol",
        "colab_type": "code",
        "outputId": "86297e83-a7a9-41a4-80b7-47eb392a4e66",
        "colab": {}
      },
      "source": [
        "#What happens when I give a strange word, and try to get its word vector in Spacy?\n",
        "temp = nlp('practicalnlp is a newword')\n",
        "temp[0].vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2PSThugoPos",
        "colab_type": "text"
      },
      "source": [
        "Well, at least, this is better than throwing an exception! :) \n",
        "\n"
      ]
    }
  ]
}