{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLNg_Puse6EX"
      },
      "source": [
        "In this notebook we will demonstrate different text classification models trained using the IMDB reviews dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eOJLveJqtEO3"
      },
      "outputs": [],
      "source": [
        "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "# !pip install numpy==1.23.5\n",
        "# !pip install wget==3.2\n",
        "# !pip install tensorflow==2.12.0\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ixb_5zcYtEO5"
      },
      "outputs": [],
      "source": [
        "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "# try:\n",
        "#     import google.colab\n",
        "#     !curl  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/ch4-requirements.txt | xargs -n 1 -L 1 pip install\n",
        "# except ModuleNotFoundError:\n",
        "#     !pip install -r \"ch4-requirements.txt\"\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xqUcb7NBb5--"
      },
      "outputs": [],
      "source": [
        "#Make the necessary imports\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import wget\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from zipfile import ZipFile\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MqW5vWwfiCP"
      },
      "source": [
        "Here we set all the paths of all the external datasets and models such as [glove](https://nlp.stanford.edu/projects/glove/) and [IMDB reviews dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HUKTqLHud7fo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "try:\n",
        "\n",
        "    from google.colab import files\n",
        "\n",
        "    !wget -P DATAPATH http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip DATAPATH/glove.6B.zip -d DATAPATH/glove.6B\n",
        "\n",
        "    !wget -P DATAPATH http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    !tar -xvf DATAPATH/aclImdb_v1.tar.gz -C DATAPATH\n",
        "\n",
        "    BASE_DIR = 'DATAPATH'\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "\n",
        "    if not os.path.exists('Data/glove.6B'):\n",
        "        os.mkdir('Data/glove.6B')\n",
        "\n",
        "        url='http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "        wget.download(url,'Data')\n",
        "\n",
        "        temp='Data/glove.6B.zip'\n",
        "        file = ZipFile(temp)\n",
        "        file.extractall('Data/glove.6B')\n",
        "        file.close()\n",
        "\n",
        "\n",
        "\n",
        "    if not os.path.exists('Data/aclImdb'):\n",
        "\n",
        "        url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "        wget.download(url,'Data')\n",
        "\n",
        "        temp='Data/aclImdb_v1.tar.gz'\n",
        "        tar = tarfile.open(temp, \"r:gz\")\n",
        "        tar.extractall('Data')\n",
        "        tar.close()\n",
        "\n",
        "    BASE_DIR = 'Data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qvl1qb78fUib"
      },
      "outputs": [],
      "source": [
        "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
        "TRAIN_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb/train')\n",
        "TEST_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Yu9xmAZEd7fp"
      },
      "outputs": [],
      "source": [
        "#Within these, I only have a pos/ and a neg/ folder containing text files\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "#started off from: https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
        "#and from: https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmifkoA8b5_N"
      },
      "source": [
        "### Loading and Preprocessing\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WI4O1usEb5_O"
      },
      "outputs": [],
      "source": [
        "#Function to load the data from the dataset into the notebook. Will be called twice - for train and test.\n",
        "def get_data(data_dir):\n",
        "    texts = []  # list of text samples\n",
        "    labels_index = {'pos':1, 'neg':0}  # dictionary mapping label name to numeric id\n",
        "    labels = []  # list of label ids\n",
        "    for name in sorted(os.listdir(data_dir)):\n",
        "        path = os.path.join(data_dir, name)\n",
        "        if os.path.isdir(path):\n",
        "            if name=='pos' or name=='neg':\n",
        "                label_id = labels_index[name]\n",
        "                for fname in sorted(os.listdir(path)):\n",
        "                        fpath = os.path.join(path, fname)\n",
        "                        text = open(fpath,encoding='utf8').read()\n",
        "                        texts.append(text)\n",
        "                        labels.append(label_id)\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = get_data(TRAIN_DATA_DIR)\n",
        "test_texts, test_labels = get_data(TEST_DATA_DIR)\n",
        "labels_index = {'pos':1, 'neg':0}\n",
        "\n",
        "#Just to see how the data looks like.\n",
        "#print(train_texts[0])\n",
        "#print(train_labels[0])\n",
        "#print(test_texts[24999])\n",
        "#print(test_labels[24999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhhqM0Jdd7fs",
        "outputId": "9e16478c-8111-4aaf-e73b-b89359dd114f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 88582 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e0V1-bBb5_d",
        "outputId": "94d409aa-5ac2-4b4a-809d-fc91d4563285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting the train data into train and valid is done\n"
          ]
        }
      ],
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "trainvalid_labels = to_categorical(np.asarray(train_labels))\n",
        "test_labels = to_categorical(np.asarray(test_labels))\n",
        "\n",
        "# split the training data into a training set and a validation set\n",
        "indices = np.arange(trainvalid_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "trainvalid_data = trainvalid_data[indices]\n",
        "trainvalid_labels = trainvalid_labels[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
        "x_train = trainvalid_data[:-num_validation_samples]\n",
        "y_train = trainvalid_labels[:-num_validation_samples]\n",
        "x_val = trainvalid_data[-num_validation_samples:]\n",
        "y_val = trainvalid_labels[-num_validation_samples:]\n",
        "#This is the data we will use for CNN and RNN training\n",
        "print('Splitting the train data into train and valid is done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUHqg2vvb5_l",
        "outputId": "0b0bc141-184a-4d99-bf55-360d0e6a0212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing embedding matrix.\n",
            "Found 400000 word vectors in Glove embeddings.\n",
            "Preparing of embedding matrix is done\n"
          ]
        }
      ],
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
        "#print(embeddings_index[\"google\"])\n",
        "\n",
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEastnX8gdxR"
      },
      "source": [
        "### 1D CNN Model with pre-trained embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTY-4K-Ob5_t",
        "outputId": "834682a9-9371-4769-a967-cc2b2c24eaa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define a 1D CNN model.\n",
            "157/157 [==============================] - 139s 878ms/step - loss: 0.6694 - acc: 0.6158 - val_loss: 0.5212 - val_acc: 0.7606\n",
            "782/782 [==============================] - 56s 71ms/step - loss: 0.5251 - acc: 0.7537\n",
            "Test accuracy with CNN: 0.7536799907684326\n"
          ]
        }
      ],
      "source": [
        "print('Define a 1D CNN model.')\n",
        "\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(embedding_layer)\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "#Train the model. Tune to validation set.\n",
        "cnnmodel.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(x_val, y_val))\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdDj2FJzgi_W"
      },
      "source": [
        "### 1D CNN model with training your own embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI0bISwRb5_w",
        "outputId": "1c2285f4-edd4-4142-8f9e-7da2f9a91dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
            "157/157 [==============================] - 216s 1s/step - loss: 0.6921 - acc: 0.5152 - val_loss: 0.6671 - val_acc: 0.6168\n",
            "782/782 [==============================] - 66s 85ms/step - loss: 0.6667 - acc: 0.6200\n",
            "Test accuracy with CNN: 0.6200399994850159\n"
          ]
        }
      ],
      "source": [
        "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "#Train the model. Tune to validation set.\n",
        "cnnmodel.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(x_val, y_val))\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GwhXpmSgt4H"
      },
      "source": [
        "### LSTM Model with training your own embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvBt2Brib5_4",
        "outputId": "434183cf-f713-4911-b403-100223907162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training an LSTM model, training embedding layer on the fly\n",
            "Training the RNN\n",
            "625/625 [==============================] - 1315s 2s/step - loss: 0.4609 - accuracy: 0.7807 - val_loss: 0.3932 - val_accuracy: 0.8286\n",
            "782/782 [==============================] - 191s 245ms/step - loss: 0.4004 - accuracy: 0.8236\n",
            "Test accuracy with RNN: 0.8235999941825867\n"
          ]
        }
      ],
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "#model\n",
        "rnnmodel = Sequential()\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJYzsZFSg9z-"
      },
      "source": [
        "### LSTM Model using pre-trained Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eymx0IyCb5_-",
        "outputId": "2c6c182a-0dee-442c-f978-ac16e840b51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training an LSTM model, using pre-trained embedding layer\n",
            "Training the RNN\n",
            "625/625 [==============================] - 1075s 2s/step - loss: 0.6050 - accuracy: 0.6728 - val_loss: 0.4578 - val_accuracy: 0.7916\n",
            "782/782 [==============================] - 183s 234ms/step - loss: 0.4554 - accuracy: 0.7917\n",
            "Test accuracy with RNN: 0.7916799783706665\n"
          ]
        }
      ],
      "source": [
        "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
        "\n",
        "rnnmodel2 = Sequential()\n",
        "rnnmodel2.add(embedding_layer)\n",
        "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel2.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel2.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb81rafef3Wl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}