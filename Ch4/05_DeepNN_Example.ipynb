{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLNg_Puse6EX"
   },
   "source": [
    "In this notebook we will demonstrate different text classification models trained using the IMDB reviews dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqUcb7NBb5--"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Make the necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import wget\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "from zipfile import ZipFile\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MqW5vWwfiCP"
   },
   "source": [
    "Here we set all the paths of all the external datasets and models such as [glove](https://nlp.stanford.edu/projects/glove/) and [IMDB reviews dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    from google.colab import files\n",
    "    \n",
    "    !wget -P DATAPATH http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip DATAPATH/glove.6B.zip -C DATAPATH\n",
    "    \n",
    "    !wget -P DATAPATH http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar -xvf DATAPATH/aclImdb_v1.tar.gz -C DATAPATH\n",
    "    \n",
    "    BASE_DIR = 'DATAPATH'\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "    \n",
    "    if not os.path.exists(os.getcwd()+'\\\\Data\\\\glove.6B'):\n",
    "        os.makdir(os.getcwd()+'\\\\Data\\\\glove.6B')\n",
    "        \n",
    "        url='http://nlp.stanford.edu/data/glove.6B.zip' \n",
    "        path=os.getcwd()+'\\Data' \n",
    "        wget.download(url,path)  \n",
    "  \n",
    "        temp=path+'\\glove.6B.zip' \n",
    "        file = ZipFile(temp)  \n",
    "        file.extractall(path+'\\glove.6B') \n",
    "        file.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "    if not os.path.exists(os.getcwd()+'\\\\Data\\\\aclImdb'):\n",
    "        url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' \n",
    "        path=os.getcwd()+'\\Data' \n",
    "        wget.download(url,path)\n",
    "        \n",
    "        temp=path+'\\aclImdb_v1.tar.gz' \n",
    "        tar = tarfile.open(temp, \"r:gz\")\n",
    "        tar.extractall(path)      \n",
    "        tar.close()\n",
    "    \n",
    "    BASE_DIR = 'Data'\n",
    "\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TRAIN_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb\\\\train')\n",
    "TEST_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb\\\\test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Within these, I only have a pos/ and a neg/ folder containing text files \n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000 \n",
    "EMBEDDING_DIM = 100 \n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "#started off from: https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "#and from: https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmifkoA8b5_N"
   },
   "source": [
    "### Loading and Preprocessing\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI4O1usEb5_O"
   },
   "outputs": [],
   "source": [
    "#Function to load the data from the dataset into the notebook. Will be called twice - for train and test.\n",
    "def get_data(data_dir):\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {'pos':1, 'neg':0}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "    for name in sorted(os.listdir(data_dir)):\n",
    "        path = os.path.join(data_dir, name)\n",
    "        if os.path.isdir(path):\n",
    "            if name=='pos' or name=='neg':\n",
    "                label_id = labels_index[name]\n",
    "                for fname in sorted(os.listdir(path)):\n",
    "                        fpath = os.path.join(path, fname)\n",
    "                        text = open(fpath,encoding='utf8').read()\n",
    "                        texts.append(text)\n",
    "                        labels.append(label_id)\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = get_data(TRAIN_DATA_DIR)\n",
    "test_texts, test_labels = get_data(TEST_DATA_DIR)\n",
    "labels_index = {'pos':1, 'neg':0} \n",
    "\n",
    "#Just to see how the data looks like. \n",
    "#print(train_texts[0])\n",
    "#print(train_labels[0])\n",
    "#print(test_texts[24999])\n",
    "#print(test_labels[24999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer \n",
    "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data. \n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) \n",
    "tokenizer.fit_on_texts(train_texts) \n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes \n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts) \n",
    "word_index = tokenizer.word_index \n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_e0V1-bBb5_d",
    "outputId": "deb3c74b-30b8-451a-cc37-49df30520da1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the train data into train and valid is done\n"
     ]
    }
   ],
   "source": [
    "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
    "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
    "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "trainvalid_labels = to_categorical(np.asarray(train_labels))\n",
    "test_labels = to_categorical(np.asarray(test_labels))\n",
    "\n",
    "# split the training data into a training set and a validation set\n",
    "indices = np.arange(trainvalid_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trainvalid_data = trainvalid_data[indices]\n",
    "trainvalid_labels = trainvalid_labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
    "x_train = trainvalid_data[:-num_validation_samples]\n",
    "y_train = trainvalid_labels[:-num_validation_samples]\n",
    "x_val = trainvalid_data[-num_validation_samples:]\n",
    "y_val = trainvalid_labels[-num_validation_samples:]\n",
    "#This is the data we will use for CNN and RNN training\n",
    "print('Splitting the train data into train and valid is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUHqg2vvb5_l",
    "outputId": "32c014cc-cee4-4a84-896b-7db39a24eb21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Found 400000 word vectors in Glove embeddings.\n",
      "Preparing of embedding matrix is done\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
    "#print(embeddings_index[\"google\"])\n",
    "\n",
    "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load these pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "print(\"Preparing of embedding matrix is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEastnX8gdxR"
   },
   "source": [
    "### 1D CNN Model with pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTY-4K-Ob5_t",
    "outputId": "22425d40-4639-476a-872d-2916e9771195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define a 1D CNN model.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 49s 2ms/step - loss: 0.6787 - acc: 0.6084 - val_loss: 0.5057 - val_acc: 0.7748\n",
      "25000/25000 [==============================] - 15s 617us/step\n",
      "Test accuracy with CNN: 0.7612800002098083\n"
     ]
    }
   ],
   "source": [
    "print('Define a 1D CNN model.')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(embedding_layer)\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VdDj2FJzgi_W"
   },
   "source": [
    "### 1D CNN model with training your own embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zI0bISwRb5_w",
    "outputId": "1e96c6be-66c2-4a59-da1f-fc80e90889bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.5023 - acc: 0.7254 - val_loss: 0.2894 - val_acc: 0.8848\n",
      "25000/25000 [==============================] - 18s 708us/step\n",
      "Test accuracy with CNN: 0.8749200105667114\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GwhXpmSgt4H"
   },
   "source": [
    "### LSTM Model with training your own embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvBt2Brib5_4",
    "outputId": "1436903e-66f2-4add-d04b-143e551c534d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training an LSTM model, training embedding layer on the fly\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training the RNN\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 445s 22ms/step - loss: 0.4829 - accuracy: 0.7736 - val_loss: 0.4104 - val_accuracy: 0.8264\n",
      "25000/25000 [==============================] - 135s 5ms/step\n",
      "Test accuracy with RNN: 0.8212599754333496\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
    "\n",
    "#model\n",
    "rnnmodel = Sequential()\n",
    "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel.add(Dense(2, activation='sigmoid'))\n",
    "rnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print('Training the RNN')\n",
    "\n",
    "rnnmodel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJYzsZFSg9z-"
   },
   "source": [
    "### LSTM Model using pre-trained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eymx0IyCb5_-",
    "outputId": "7d665d21-4f34-46bb-ac3a-2fab1055268b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training an LSTM model, using pre-trained embedding layer\n",
      "Training the RNN\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 349s 17ms/step - loss: 0.6123 - accuracy: 0.6603 - val_loss: 0.4820 - val_accuracy: 0.7859\n",
      "25000/25000 [==============================] - 131s 5ms/step\n",
      "Test accuracy with RNN: 0.7855600118637085\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
    "\n",
    "rnnmodel2 = Sequential()\n",
    "rnnmodel2.add(embedding_layer)\n",
    "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel2.add(Dense(2, activation='sigmoid'))\n",
    "rnnmodel2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print('Training the RNN')\n",
    "\n",
    "rnnmodel2.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DeepNN_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
