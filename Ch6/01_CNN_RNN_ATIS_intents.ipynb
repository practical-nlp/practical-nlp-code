{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNVXrlK3GkDL"
      },
      "source": [
        "## Intent Detection Using CNN & RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz0jLNgTnMwF"
      },
      "source": [
        "In this notebook we demonstrate various CNN and RNN architectures for the task of intent detection on the ATIS dataset. The ATIS dataset is a standard benchmark dataset for the tast of intent detection. ATIS Stands for Airline Travel Information System. The dataset can we found in the `Data2` folder under the `Data` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. For this notebook you will need some files from the Data directory present in the Ch6 Folder. So you can follow below steps to load the whole directory here.\n",
        "\n",
        "1. Create a zip file of the Data directory.\n",
        "2. Upload it here on colabs as a file.\n",
        "3. Extract the files using the command 'unzip Data.zip' (considering that you named the zip file as Data.zip)\n",
        "\n",
        "## B. Another file which needs to be downloaded is from Kaggle, download 'glove6b.100d.txt' only using below link, and upload and extract zip file as above.\n",
        "https://www.kaggle.com/datasets/anindya2906/glove6b\n",
        "\n",
        "# (OR)\n",
        "\n",
        "## Just run below cell (May take time to download)"
      ],
      "metadata": {
        "id": "uMGP-yynL8Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OQnuaefRjzD",
        "outputId": "187524b7-389d-43b1-faa9-b2d3e353052d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enRfuapQNLqJ",
        "outputId": "c0998e4b-235e-4101-e136-c330a4338e04"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Data.zip\n",
            "   creating: Data/\n",
            "  inflating: Data/utils.py           \n",
            "   creating: Data/CoNLL - 2003/\n",
            "   creating: Data/snips/\n",
            "   creating: Data/data/\n",
            "   creating: Data/data2/\n",
            "  inflating: Data/utils.pyc          \n",
            "   creating: Data/CoNLL - 2003/en/\n",
            "  inflating: Data/snips/validate_PlayMusic.json  \n",
            "  inflating: Data/snips/train_PlayMusic_full.json  \n",
            "  inflating: Data/data/atis.test.pkl  \n",
            "  inflating: Data/data/atis.fold1.pkl  \n",
            "  inflating: Data/data/atis.fold0.pkl  \n",
            "  inflating: Data/data/atis.fold2.pkl  \n",
            "  inflating: Data/data/atis.fold3.pkl  \n",
            "  inflating: Data/data/atis.train.pkl  \n",
            "  inflating: Data/data/atis.fold4.pkl  \n",
            "  inflating: Data/data2/atis.train.w-intent.iob  \n",
            "  inflating: Data/data2/atis.test.w-intent.iob  \n",
            "  inflating: Data/data2/atis-2.train.w-intent.iob  \n",
            "  inflating: Data/data2/atis-2.dev.w-intent.iob  \n",
            "  inflating: Data/CoNLL - 2003/en/train.txt  \n",
            "  inflating: Data/CoNLL - 2003/en/valid.txt  \n",
            "  inflating: Data/CoNLL - 2003/en/test.txt  \n",
            "  inflating: Data/CoNLL - 2003/en/metadata  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove"
      ],
      "metadata": {
        "id": "B0bBa7oBQ8ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrwnLmqfGkDN"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T02:51:26.312446Z",
          "start_time": "2020-01-21T02:51:15.878759Z"
        },
        "id": "7GnbW58Yi_7P"
      },
      "outputs": [],
      "source": [
        "#general imports\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "random.seed(0) #for reproducability of results\n",
        "\n",
        "#basic imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#NN imports\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "\n",
        "#encoder\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGFhMpnjo4L1"
      },
      "source": [
        "## Data Loading\n",
        "We load the data with the help of a few functions from `utils.py` which is included in this repository's Ch6 folder under folder name \"Data\".\n",
        "### Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:16:45.725380Z",
          "start_time": "2020-01-21T03:16:45.669373Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl5A0Kwji_7h",
        "outputId": "20f24a86-d746-43fd-bc1e-a4590ccbf486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences : 4952\n",
            "Number of unique intents : 17\n",
            "('i want to fly from boston at 838 am and arrive in denver at 1110 in the morning', 'atis_flight')\n",
            "('what flights are available from pittsburgh to baltimore on thursday morning', 'atis_flight')\n",
            "('what is the arrival time in san francisco for the 755 am flight leaving washington', 'atis_flight_time')\n",
            "('cheapest airfare from tacoma to orlando', 'atis_airfare')\n",
            "('round trip fares from pittsburgh to philadelphia under 1000 dollars', 'atis_airfare')\n"
          ]
        }
      ],
      "source": [
        "from utils import fetch_data, read_method\n",
        "\n",
        "sents,labels,intents = fetch_data('Data/data2/atis.train.w-intent.iob')\n",
        "\n",
        "train_sentences = [\" \".join(i) for i in sents]\n",
        "\n",
        "train_texts = train_sentences\n",
        "train_labels= intents.tolist()\n",
        "\n",
        "vals = []\n",
        "\n",
        "for i in range(len(train_labels)):\n",
        "    if \"#\" in train_labels[i]:\n",
        "        vals.append(i)\n",
        "\n",
        "for i in vals[::-1]:\n",
        "    train_labels.pop(i)\n",
        "    train_texts.pop(i)\n",
        "\n",
        "print (\"Number of training sentences :\",len(train_texts))\n",
        "print (\"Number of unique intents :\",len(set(train_labels)))\n",
        "\n",
        "for i in zip(train_texts[:5], train_labels[:5]):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qckNEPKRo8_V"
      },
      "source": [
        "### Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:16:53.184193Z",
          "start_time": "2020-01-21T03:16:52.523898Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQynEfVmi_7m",
        "outputId": "d55009d5-ac70-496c-c957-5a9b4d951762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "atis_day_name\n",
            "atis_day_name\n",
            "Number of testing sentences : 876\n",
            "Number of unique intents : 15\n",
            "('i would like to find a flight from charlotte to las vegas that makes a stop in st. louis', 'atis_flight')\n",
            "('on april first i need a ticket from tacoma to san jose departing before 7 am', 'atis_airfare')\n",
            "('on april first i need a flight going from phoenix to san diego', 'atis_flight')\n",
            "('i would like a flight traveling one way from phoenix to san diego on april first', 'atis_flight')\n",
            "('i would like a flight from orlando to salt lake city for april first on delta airlines', 'atis_flight')\n"
          ]
        }
      ],
      "source": [
        "from utils import fetch_data, read_method\n",
        "\n",
        "sents,labels,intents = fetch_data('Data/data2/atis.test.w-intent.iob')\n",
        "\n",
        "test_sentences = [\" \".join(i) for i in sents]\n",
        "\n",
        "test_texts = test_sentences\n",
        "test_labels = intents.tolist()\n",
        "\n",
        "new_labels = set(test_labels) - set(train_labels)\n",
        "\n",
        "vals = []\n",
        "\n",
        "for i in range(len(test_labels)):\n",
        "    if \"#\" in test_labels[i]:\n",
        "        vals.append(i)\n",
        "    elif test_labels[i] in new_labels:\n",
        "        print(test_labels[i])\n",
        "        vals.append(i)\n",
        "\n",
        "for i in vals[::-1]:\n",
        "    test_labels.pop(i)\n",
        "    test_texts.pop(i)\n",
        "\n",
        "print (\"Number of testing sentences :\",len(test_texts))\n",
        "print (\"Number of unique intents :\",len(set(test_labels)))\n",
        "\n",
        "for i in zip(test_texts[:5], test_labels[:5]):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUZsI3ZmpBA2"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Nu_2T-u6GkDQ"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 300\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:16:58.708150Z",
          "start_time": "2020-01-21T03:16:58.445025Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk1EXJANi_7p",
        "outputId": "ddb3c77d-8d68-4161-c935-4fbe0d3e7543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 897 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:17:08.464993Z",
          "start_time": "2020-01-21T03:17:08.455586Z"
        },
        "id": "PyTlYCLQi_76"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(train_labels)\n",
        "train_labels = le.transform(train_labels)\n",
        "test_labels = le.transform(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:18:09.843555Z",
          "start_time": "2020-01-21T03:18:09.802336Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPdDoAAli_8K",
        "outputId": "151a4443-44be-4ff2-b136-8ef2b271cfc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting the train data into train and valid is done\n"
          ]
        }
      ],
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        " #initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "trainvalid_labels = to_categorical(train_labels)\n",
        "\n",
        "test_labels = to_categorical(np.asarray(test_labels), num_classes= trainvalid_labels.shape[1])\n",
        "\n",
        "# split the training data into a training set and a validation set\n",
        "indices = np.arange(trainvalid_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "trainvalid_data = trainvalid_data[indices]\n",
        "trainvalid_labels = trainvalid_labels[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
        "x_train = trainvalid_data[:-num_validation_samples]\n",
        "y_train = trainvalid_labels[:-num_validation_samples]\n",
        "x_val = trainvalid_data[-num_validation_samples:]\n",
        "y_val = trainvalid_labels[-num_validation_samples:]\n",
        "#This is the data we will use for CNN and RNN training\n",
        "print('Splitting the train data into train and valid is done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulQtiGV7GkDR"
      },
      "source": [
        "## Modeling\n",
        "### Embedding Matrix\n",
        "We need to prepare our embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:18:41.097840Z",
          "start_time": "2020-01-21T03:18:14.824841Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSKw8NHxi_8O",
        "outputId": "7182c9be-6ee4-4c17-f6ac-32d2fb81e746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing embedding matrix.\n",
            "Found 400000 word vectors in Glove embeddings.\n",
            "Preparing of embedding matrix is done\n"
          ]
        }
      ],
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "\n",
        "# Download GloVe 6B from here: https://nlp.stanford.edu/projects/glove/\n",
        "BASE_DIR = '.'\n",
        "# GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(BASE_DIR, 'glove.6B.100d.txt'), encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
        "#print(embeddings_index[\"google\"])\n",
        "\n",
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2yiEudUGkDS"
      },
      "source": [
        "### CNN with Pre-Trained Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:19:56.592225Z",
          "start_time": "2020-01-21T03:19:49.795954Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1sdtUNYi_8T",
        "outputId": "a5c6a4b0-2f1d-4d92-fb75-ee9e70513048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define a 1D CNN model.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 100)          89800     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 296, 128)          64128     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 59, 128)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 55, 128)           82048     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 11, 128)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 7, 128)            82048     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 17)                2193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 336,729\n",
            "Trainable params: 246,929\n",
            "Non-trainable params: 89,800\n",
            "_________________________________________________________________\n",
            "28/28 [==============================] - 15s 462ms/step - loss: 1.1631 - acc: 0.7176 - val_loss: 0.8967 - val_acc: 0.7333\n",
            "28/28 [==============================] - 1s 34ms/step - loss: 1.0032 - acc: 0.7215\n",
            "Test accuracy with CNN: 0.7214611768722534\n"
          ]
        }
      ],
      "source": [
        "print('Define a 1D CNN model.')\n",
        "\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(embedding_layer)\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(len(trainvalid_labels[0]), activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "cnnmodel.summary()\n",
        "\n",
        "#Train the model. Tune to validation set.\n",
        "cnnmodel.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(x_val, y_val))\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHuW6Pz-GkDS"
      },
      "source": [
        "### CNN-Embedding Layer\n",
        "Here, we train a CNN model with an embedding layer which is being trained on the fly instead of using the pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:20:48.874312Z",
          "start_time": "2020-01-21T03:20:39.637994Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekMBb2Ski_8j",
        "outputId": "a580efb6-eac0-43ec-83cc-7c5d4c11a7be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 17)                2193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,824,849\n",
            "Trainable params: 2,824,849\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "28/28 [==============================] - 22s 743ms/step - loss: 1.3620 - acc: 0.7188 - val_loss: 1.4628 - val_acc: 0.7333\n",
            "28/28 [==============================] - 1s 40ms/step - loss: 1.7164 - acc: 0.7215\n",
            "Test accuracy with CNN: 0.7214611768722534\n"
          ]
        }
      ],
      "source": [
        "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(len(trainvalid_labels[0]), activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "cnnmodel.summary()\n",
        "\n",
        "#Train the model. Tune to validation set.\n",
        "cnnmodel.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(x_val, y_val))\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPREer4GGkDT"
      },
      "source": [
        "### RNN-Embedding Layer\n",
        "Here, we train a RNN model with an embedding layer which is being trained on the fly instead of using the pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:23:06.806061Z",
          "start_time": "2020-01-21T03:21:34.339598Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE5578-ei_8m",
        "outputId": "86d49619-e126-48ff-b364-0316076ba22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training an LSTM model, training embedding layer on the fly\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               131584    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 17)                2193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,693,777\n",
            "Trainable params: 2,693,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training the RNN\n",
            "109/109 [==============================] - 126s 1s/step - loss: 0.1649 - accuracy: 0.7352 - val_loss: 0.1025 - val_accuracy: 0.7333\n",
            "28/28 [==============================] - 5s 166ms/step - loss: 0.1145 - accuracy: 0.7215\n",
            "Test accuracy with RNN: 0.7214611768722534\n"
          ]
        }
      ],
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "#modified from:\n",
        "\n",
        "rnnmodel = Sequential()\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(len(trainvalid_labels[0]), activation='sigmoid'))\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "rnnmodel.summary()\n",
        "\n",
        "print('Training the RNN')\n",
        "rnnmodel.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glsjrccyGkDT"
      },
      "source": [
        "### LSTM with Pre-Trained Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-21T03:25:33.571529Z",
          "start_time": "2020-01-21T03:24:04.513325Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTTJqKFHi_8x",
        "outputId": "4edc85c6-e8be-46ba-f72c-11f7f3f4b605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training an LSTM model, using pre-trained embedding layer\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 100)          89800     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               117248    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 17)                2193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 209,241\n",
            "Trainable params: 119,441\n",
            "Non-trainable params: 89,800\n",
            "_________________________________________________________________\n",
            "Training the RNN\n",
            "109/109 [==============================] - 104s 929ms/step - loss: 0.1473 - accuracy: 0.7286 - val_loss: 0.0983 - val_accuracy: 0.7333\n",
            "28/28 [==============================] - 3s 110ms/step - loss: 0.1096 - accuracy: 0.7215\n",
            "Test accuracy with RNN: 0.7214611768722534\n"
          ]
        }
      ],
      "source": [
        "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
        "\n",
        "#modified from:\n",
        "\n",
        "rnnmodel2 = Sequential()\n",
        "rnnmodel2.add(embedding_layer)\n",
        "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel2.add(Dense(len(trainvalid_labels[0]), activation='sigmoid'))\n",
        "rnnmodel2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "rnnmodel2.summary()\n",
        "\n",
        "print('Training the RNN')\n",
        "rnnmodel2.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "zTlSMklki_83"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}