{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Detection Using CNN & RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wz0jLNgTnMwF"
   },
   "source": [
    "In this notebook we demonstrate various CNN and RNN architectures for the task of intent detection on the ATIS dataset. The ATIS dataset is a standard benchmark dataset for the tast of intent detection. ATIS Stands for Airline Travel Information System. The dataset can we found in the `Data2` folder under the `Data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T02:51:26.312446Z",
     "start_time": "2020-01-21T02:51:15.878759Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7GnbW58Yi_7P",
    "outputId": "67fd2a56-575c-4f98-9628-aae205443f8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#general imports\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "random.seed(0) #for reproducability of results\n",
    "\n",
    "#basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#NN imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "\n",
    "#encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGFhMpnjo4L1"
   },
   "source": [
    "## Data Loading\n",
    "We load the data with the help of a few functions from `utils.py` which is included in this repository's Ch6 folder under folder name \"Data\". \n",
    "### Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:16:45.725380Z",
     "start_time": "2020-01-21T03:16:45.669373Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pl5A0Kwji_7h",
    "outputId": "e7f3b18e-0d39-4428-957b-4e247f3c192b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences : 4952\n",
      "Number of unique intents : 17\n",
      "('i want to fly from boston at 838 am and arrive in denver at 1110 in the morning', 'atis_flight')\n",
      "('what flights are available from pittsburgh to baltimore on thursday morning', 'atis_flight')\n",
      "('what is the arrival time in san francisco for the 755 am flight leaving washington', 'atis_flight_time')\n",
      "('cheapest airfare from tacoma to orlando', 'atis_airfare')\n",
      "('round trip fares from pittsburgh to philadelphia under 1000 dollars', 'atis_airfare')\n"
     ]
    }
   ],
   "source": [
    "from Data.utils import fetch_data, read_method\n",
    "\n",
    "sents,labels,intents = fetch_data('Data/data2/atis.train.w-intent.iob')\n",
    "\n",
    "train_sentences = [\" \".join(i) for i in sents]\n",
    "\n",
    "train_texts = train_sentences\n",
    "train_labels= intents.tolist()\n",
    "\n",
    "vals = []\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    if \"#\" in train_labels[i]:\n",
    "        vals.append(i)\n",
    "        \n",
    "for i in vals[::-1]:\n",
    "    train_labels.pop(i)\n",
    "    train_texts.pop(i)\n",
    "\n",
    "print (\"Number of training sentences :\",len(train_texts))\n",
    "print (\"Number of unique intents :\",len(set(train_labels)))\n",
    "\n",
    "for i in zip(train_texts[:5], train_labels[:5]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qckNEPKRo8_V"
   },
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:16:53.184193Z",
     "start_time": "2020-01-21T03:16:52.523898Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zQynEfVmi_7m",
    "outputId": "38ecc729-f2de-4500-e594-ad4d073dacdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atis_day_name\n",
      "atis_day_name\n",
      "Number of testing sentences : 876\n",
      "Number of unique intents : 15\n",
      "('i would like to find a flight from charlotte to las vegas that makes a stop in st. louis', 'atis_flight')\n",
      "('on april first i need a ticket from tacoma to san jose departing before 7 am', 'atis_airfare')\n",
      "('on april first i need a flight going from phoenix to san diego', 'atis_flight')\n",
      "('i would like a flight traveling one way from phoenix to san diego on april first', 'atis_flight')\n",
      "('i would like a flight from orlando to salt lake city for april first on delta airlines', 'atis_flight')\n"
     ]
    }
   ],
   "source": [
    "from Data.utils import fetch_data, read_method\n",
    "\n",
    "sents,labels,intents = fetch_data('Data/data2/atis.test.w-intent.iob')\n",
    "\n",
    "test_sentences = [\" \".join(i) for i in sents]\n",
    "\n",
    "test_texts = test_sentences\n",
    "test_labels = intents.tolist()\n",
    "\n",
    "new_labels = set(test_labels) - set(train_labels)\n",
    "\n",
    "vals = []\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if \"#\" in test_labels[i]:\n",
    "        vals.append(i)\n",
    "    elif test_labels[i] in new_labels:\n",
    "        print(test_labels[i])\n",
    "        vals.append(i)\n",
    "        \n",
    "for i in vals[::-1]:\n",
    "    test_labels.pop(i)\n",
    "    test_texts.pop(i)\n",
    "\n",
    "print (\"Number of testing sentences :\",len(test_texts))\n",
    "print (\"Number of unique intents :\",len(set(test_labels)))\n",
    "\n",
    "for i in zip(test_texts[:5], test_labels[:5]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUZsI3ZmpBA2"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300\n",
    "MAX_NUM_WORDS = 20000 \n",
    "EMBEDDING_DIM = 100 \n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:16:58.708150Z",
     "start_time": "2020-01-21T03:16:58.445025Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sk1EXJANi_7p",
    "outputId": "a89ff82e-7cda-4fa3-84c2-eb38679ff881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 897 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:17:08.464993Z",
     "start_time": "2020-01-21T03:17:08.455586Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PyTlYCLQi_76"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:18:09.843555Z",
     "start_time": "2020-01-21T03:18:09.802336Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XPdDoAAli_8K",
    "outputId": "ee82e0a6-39a1-4893-b9ea-a45db8f30236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the train data into train and valid is done\n"
     ]
    }
   ],
   "source": [
    "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
    " #initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
    "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "trainvalid_labels = to_categorical(train_labels)\n",
    "\n",
    "test_labels = to_categorical(np.asarray(test_labels), num_classes= trainvalid_labels.shape[1])\n",
    "\n",
    "# split the training data into a training set and a validation set\n",
    "indices = np.arange(trainvalid_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trainvalid_data = trainvalid_data[indices]\n",
    "trainvalid_labels = trainvalid_labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
    "x_train = trainvalid_data[:-num_validation_samples]\n",
    "y_train = trainvalid_labels[:-num_validation_samples]\n",
    "x_val = trainvalid_data[-num_validation_samples:]\n",
    "y_val = trainvalid_labels[-num_validation_samples:]\n",
    "#This is the data we will use for CNN and RNN training\n",
    "print('Splitting the train data into train and valid is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Embedding Matrix\n",
    "We need to prepare our embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:18:41.097840Z",
     "start_time": "2020-01-21T03:18:14.824841Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gSKw8NHxi_8O",
    "outputId": "fb7a5b97-883c-473f-e016-be984bf4f194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Found 400001 word vectors in Glove embeddings.\n",
      "Preparing of embedding matrix is done\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "# Download GloVe 6B from here: https://nlp.stanford.edu/projects/glove/\n",
    "BASE_DIR = 'Data'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
    "#print(embeddings_index[\"google\"])\n",
    "\n",
    "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load these pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "print(\"Preparing of embedding matrix is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:19:56.592225Z",
     "start_time": "2020-01-21T03:19:49.795954Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "w1sdtUNYi_8T",
    "outputId": "bc393907-c69f-4371-8214-ad85f716dbc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define a 1D CNN model.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          89800     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 59, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 55, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 7, 128)            82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 336,729\n",
      "Trainable params: 246,929\n",
      "Non-trainable params: 89,800\n",
      "_________________________________________________________________\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 1.1362 - acc: 0.7165 - val_loss: 0.7829 - val_acc: 0.8047\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.9646 - acc: 0.7511\n",
      "Test accuracy with CNN: 0.7511415481567383\n"
     ]
    }
   ],
   "source": [
    "print('Define a 1D CNN model.')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(embedding_layer)\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(trainvalid_labels[0]), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "cnnmodel.summary()\n",
    "\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-Embedding Layer\n",
    "Here, we train a CNN model with an embedding layer which is being trained on the fly instead of using the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:20:48.874312Z",
     "start_time": "2020-01-21T03:20:39.637994Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ekMBb2Ski_8j",
    "outputId": "c6f2c28d-6e5f-4d69-85b7-b708d9c8475c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 2,824,849\n",
      "Trainable params: 2,824,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "28/28 [==============================] - 6s 203ms/step - loss: 1.3051 - acc: 0.7303 - val_loss: 0.9104 - val_acc: 0.7603\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 1.1305 - acc: 0.7215\n",
      "Test accuracy with CNN: 0.7214611768722534\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(trainvalid_labels[0]), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "cnnmodel.summary()\n",
    "\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN-Embedding Layer\n",
    "Here, we train a RNN model with an embedding layer which is being trained on the fly instead of using the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:23:06.806061Z",
     "start_time": "2020-01-21T03:21:34.339598Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lE5578-ei_8m",
    "outputId": "2d245c0d-fbdf-4986-c29e-133ce5ab732d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training an LSTM model, training embedding layer on the fly\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 2,693,777\n",
      "Trainable params: 2,693,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the RNN\n",
      "109/109 [==============================] - 49s 454ms/step - loss: 0.1642 - accuracy: 0.7179 - val_loss: 0.0954 - val_accuracy: 0.7603\n",
      "28/28 [==============================] - 2s 54ms/step - loss: 0.1146 - accuracy: 0.7215\n",
      "Test accuracy with RNN: 0.7214611768722534\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
    "\n",
    "#modified from: \n",
    "\n",
    "rnnmodel = Sequential()\n",
    "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel.add(Dense(len(trainvalid_labels[0]), activation='sigmoid'))\n",
    "rnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnnmodel.summary()\n",
    "\n",
    "print('Training the RNN')\n",
    "rnnmodel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:33.571529Z",
     "start_time": "2020-01-21T03:24:04.513325Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tTTJqKFHi_8x",
    "outputId": "d5514ea5-26d4-4c9d-de30-abf6d3adb8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training an LSTM model, using pre-trained embedding layer\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          89800     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 209,241\n",
      "Trainable params: 119,441\n",
      "Non-trainable params: 89,800\n",
      "_________________________________________________________________\n",
      "Training the RNN\n",
      "109/109 [==============================] - 42s 381ms/step - loss: 0.1437 - accuracy: 0.7119 - val_loss: 0.0830 - val_accuracy: 0.7603\n",
      "28/28 [==============================] - 1s 52ms/step - loss: 0.0953 - accuracy: 0.7215\n",
      "Test accuracy with RNN: 0.7214611768722534\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
    "\n",
    "#modified from: \n",
    "\n",
    "rnnmodel2 = Sequential()\n",
    "rnnmodel2.add(embedding_layer)\n",
    "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel2.add(Dense(len(trainvalid_labels[0]), activation='sigmoid'))\n",
    "rnnmodel2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnnmodel2.summary()\n",
    "\n",
    "print('Training the RNN')\n",
    "rnnmodel2.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTlSMklki_83"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CNN_RNN_ATIS_intents.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
