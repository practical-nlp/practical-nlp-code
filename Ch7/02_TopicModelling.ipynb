{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "This notebook contains a demo of LDA and LSA using the gensim library. The dataset's link can be found in the `BookSummaries_Link.md` file under the Data folder in Ch7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.6.3)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.20.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Import OS \n",
    "import os\n",
    "# For NLTK virtual environments are high recommended and it requires python verisions higher than 3.5\n",
    "!pip install gensim\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/etherealenvy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.007026231, 'life'),\n",
      "   (0.0060395496, 'love'),\n",
      "   (0.006031805, 'family'),\n",
      "   (0.0060214815, 'father'),\n",
      "   (0.0059033865, 'he'),\n",
      "   (0.0056308284, 'novel'),\n",
      "   (0.004568777, 'young'),\n",
      "   (0.0045522423, 'she'),\n",
      "   (0.004382369, 'story'),\n",
      "   (0.004264761, 'one'),\n",
      "   (0.0042424756, 'also'),\n",
      "   (0.004066663, 'in'),\n",
      "   (0.0040207854, 'mother'),\n",
      "   (0.0038233518, 'two'),\n",
      "   (0.0037485228, 'becomes'),\n",
      "   (0.0036681942, 'time'),\n",
      "   (0.0036503694, 'first'),\n",
      "   (0.0035617696, 'new'),\n",
      "   (0.0035586786, 'years'),\n",
      "   (0.003553522, 'son')],\n",
      "  -0.9155499921177572),\n",
      " ([(0.007956107, 'he'),\n",
      "   (0.0074816775, 'she'),\n",
      "   (0.0064061387, 'mother'),\n",
      "   (0.005793881, 'one'),\n",
      "   (0.005216782, 'tells'),\n",
      "   (0.005172581, 'back'),\n",
      "   (0.004947877, 'house'),\n",
      "   (0.004940073, 'father'),\n",
      "   (0.004542262, 'school'),\n",
      "   (0.0045250333, 'go'),\n",
      "   (0.0044640102, 'home'),\n",
      "   (0.004325223, 'they'),\n",
      "   (0.0041368674, 'day'),\n",
      "   (0.004133858, 'family'),\n",
      "   (0.003949343, 'get'),\n",
      "   (0.0039093546, 'when'),\n",
      "   (0.0038658774, 'find'),\n",
      "   (0.0036393076, 'man'),\n",
      "   (0.0035100677, 'goes'),\n",
      "   (0.003503199, 'new')],\n",
      "  -0.9323329589336468),\n",
      " ([(0.0072388155, 'he'),\n",
      "   (0.0057483213, 'one'),\n",
      "   (0.0043527302, 'murder'),\n",
      "   (0.003839126, 'police'),\n",
      "   (0.0037286696, 'man'),\n",
      "   (0.0031914695, 'two'),\n",
      "   (0.003018892, 'case'),\n",
      "   (0.0029713218, 'also'),\n",
      "   (0.0029704147, 'would'),\n",
      "   (0.0028666044, 'time'),\n",
      "   (0.0026530696, 'she'),\n",
      "   (0.0025573473, 'wife'),\n",
      "   (0.0025425071, 'death'),\n",
      "   (0.002508297, 'house'),\n",
      "   (0.002502906, 'back'),\n",
      "   (0.0024625757, 'found'),\n",
      "   (0.0024601123, 'in'),\n",
      "   (0.0023955721, 'later'),\n",
      "   (0.002374006, 'old'),\n",
      "   (0.0022676678, 'when')],\n",
      "  -1.0109854267701848),\n",
      " ([(0.0070246486, 'earth'),\n",
      "   (0.0062227994, 'one'),\n",
      "   (0.005164357, 'time'),\n",
      "   (0.005123321, 'human'),\n",
      "   (0.004958949, 'world'),\n",
      "   (0.0044332608, 'new'),\n",
      "   (0.0040380103, 'planet'),\n",
      "   (0.0036678964, 'life'),\n",
      "   (0.0034522717, 'space'),\n",
      "   (0.0034378455, 'he'),\n",
      "   (0.003330606, 'also'),\n",
      "   (0.0031583216, 'humans'),\n",
      "   (0.0029947327, 'two'),\n",
      "   (0.0029877145, 'people'),\n",
      "   (0.002912987, 'book'),\n",
      "   (0.002908064, 'ship'),\n",
      "   (0.0029070321, 'years'),\n",
      "   (0.0028552404, 'in'),\n",
      "   (0.0027980315, 'story'),\n",
      "   (0.002759131, 'first')],\n",
      "  -1.1151968633691758),\n",
      " ([(0.0056709945, 'he'),\n",
      "   (0.0049731904, 'they'),\n",
      "   (0.0048136674, 'one'),\n",
      "   (0.0044813026, 'back'),\n",
      "   (0.00435041, 'find'),\n",
      "   (0.004032603, 'ship'),\n",
      "   (0.0036065145, 'king'),\n",
      "   (0.0034282503, 'two'),\n",
      "   (0.0033809398, 'city'),\n",
      "   (0.0033805675, 'help'),\n",
      "   (0.0033219962, 'battle'),\n",
      "   (0.003221685, 'kill'),\n",
      "   (0.0031827132, 'after'),\n",
      "   (0.0030648233, 'group'),\n",
      "   (0.0030225348, 'also'),\n",
      "   (0.0030159985, 'however'),\n",
      "   (0.00295473, 'time'),\n",
      "   (0.0029474085, 'escape'),\n",
      "   (0.0029190017, 'way'),\n",
      "   (0.002840756, 'army')],\n",
      "  -1.172261573774359),\n",
      " ([(0.010040424, 'book'),\n",
      "   (0.00852043, 'war'),\n",
      "   (0.005588232, 'in'),\n",
      "   (0.005565338, 'world'),\n",
      "   (0.0047673965, 'novel'),\n",
      "   (0.004560236, 'states'),\n",
      "   (0.004480826, 'also'),\n",
      "   (0.004236, 'new'),\n",
      "   (0.004136615, 'chapter'),\n",
      "   (0.0040763384, 'story'),\n",
      "   (0.003671584, 'first'),\n",
      "   (0.003642358, 'he'),\n",
      "   (0.003577084, 'people'),\n",
      "   (0.0034678685, 'united'),\n",
      "   (0.0034178188, 'american'),\n",
      "   (0.0033940948, 'one'),\n",
      "   (0.0030054823, 'it'),\n",
      "   (0.002890448, 'life'),\n",
      "   (0.0028572693, 'political'),\n",
      "   (0.0028462147, 'government')],\n",
      "  -1.4437669787323264),\n",
      " ([(0.006052569, 'will'),\n",
      "   (0.0055898945, 'jason'),\n",
      "   (0.0048732487, 'he'),\n",
      "   (0.004577253, 'vampire'),\n",
      "   (0.0045595895, 'king'),\n",
      "   (0.0044010202, 'leo'),\n",
      "   (0.0042045387, 'one'),\n",
      "   (0.003574945, 'new'),\n",
      "   (0.0035245384, 'in'),\n",
      "   (0.0034739294, 'world'),\n",
      "   (0.0033326591, 'time'),\n",
      "   (0.0031586662, 'first'),\n",
      "   (0.0030567679, 'story'),\n",
      "   (0.00301604, 'also'),\n",
      "   (0.0029370775, 'she'),\n",
      "   (0.00271739, 'novel'),\n",
      "   (0.0026821163, 'book'),\n",
      "   (0.002682078, 'life'),\n",
      "   (0.0024072046, 'vampires'),\n",
      "   (0.0023908913, 'two')],\n",
      "  -1.5714166184331688),\n",
      " ([(0.004763395, 'jake'),\n",
      "   (0.00472853, 'charlie'),\n",
      "   (0.0046457024, 'she'),\n",
      "   (0.0043276856, 'he'),\n",
      "   (0.004296831, 'one'),\n",
      "   (0.004022938, 'roger'),\n",
      "   (0.0039675953, 'back'),\n",
      "   (0.0038940886, 'they'),\n",
      "   (0.0038319954, 'luke'),\n",
      "   (0.003704946, 'new'),\n",
      "   (0.0036650654, 'lee'),\n",
      "   (0.003580873, 'get'),\n",
      "   (0.0034684772, 'also'),\n",
      "   (0.003467605, 'tells'),\n",
      "   (0.003334168, 'find'),\n",
      "   (0.0033315911, 'otto'),\n",
      "   (0.0030934892, 'after'),\n",
      "   (0.0030900051, 'go'),\n",
      "   (0.0030855995, 'time'),\n",
      "   (0.0030376138, 'rae')],\n",
      "  -3.1055958547248053),\n",
      " ([(0.013334864, 'jacky'),\n",
      "   (0.0056387056, 'dahlia'),\n",
      "   (0.0053249863, 'novel'),\n",
      "   (0.005012035, 'one'),\n",
      "   (0.004467886, 'story'),\n",
      "   (0.0043066693, 'also'),\n",
      "   (0.004208748, 'book'),\n",
      "   (0.003860799, 'team'),\n",
      "   (0.003778702, 'narrator'),\n",
      "   (0.003399161, 'jeremy'),\n",
      "   (0.0032797582, 'he'),\n",
      "   (0.003198044, 'rachel'),\n",
      "   (0.0031538995, 'time'),\n",
      "   (0.0030627472, 'toby'),\n",
      "   (0.00294123, 'brooke'),\n",
      "   (0.002879754, 'in'),\n",
      "   (0.002818892, 'life'),\n",
      "   (0.002736004, 'a'),\n",
      "   (0.002695834, 'back'),\n",
      "   (0.0026652121, 'years')],\n",
      "  -3.5084402554840572),\n",
      " ([(0.0064593796, 'alex'),\n",
      "   (0.0053772116, 'one'),\n",
      "   (0.004790443, 'henry'),\n",
      "   (0.0041229716, 'freddy'),\n",
      "   (0.0040125586, 'time'),\n",
      "   (0.0037541073, 'simon'),\n",
      "   (0.003718671, 'he'),\n",
      "   (0.0036895906, 'luce'),\n",
      "   (0.0035881964, 'new'),\n",
      "   (0.0035696137, 'find'),\n",
      "   (0.0035126554, 'kate'),\n",
      "   (0.0033889045, 'matt'),\n",
      "   (0.0031908199, 'elias'),\n",
      "   (0.0031363587, 'two'),\n",
      "   (0.002957296, 'thomas'),\n",
      "   (0.0028559703, 'victor'),\n",
      "   (0.0028273899, 'story'),\n",
      "   (0.0028175907, 'rowan'),\n",
      "   (0.002784071, 'also'),\n",
      "   (0.0027726132, 'fbi')],\n",
      "  -5.593241930571481)]\n"
     ]
    }
   ],
   "source": [
    "#tokenize, remove stopwords, non-alphabetic words, lowercase\n",
    "def preprocess(textstring):\n",
    "   stops =  set(stopwords.words('english'))\n",
    "   tokens = word_tokenize(textstring)\n",
    "   return [token.lower() for token in tokens if token.isalpha() and token not in stops]\n",
    "\n",
    "# This is a sample path of your downloaded data set. This is currently set to a windows based path . \n",
    "# Please update it to your actual download path regradless of your choice of operating system \n",
    "\n",
    "data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),data)\n",
    "\n",
    "summaries = []\n",
    "for line in open(data_path, encoding=\"utf-8\"):\n",
    "   temp = line.split(\"\\t\")\n",
    "   summaries.append(preprocess(temp[6]))\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "\n",
    "dictionary = Dictionary(summaries)\n",
    "\n",
    "# Filter infrequent or too frequent words.\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(summary) for summary in summaries]\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "#Train the topic model\n",
    "\n",
    "model = LdaModel(corpus=corpus, id2word=id2word,iterations=400, num_topics=10)\n",
    "top_topics = list(model.top_topics(corpus))\n",
    "pprint(top_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.013*\"jacky\" + 0.006*\"dahlia\" + 0.005*\"novel\" + 0.005*\"one\" + 0.004*\"story\" + 0.004*\"also\" + 0.004*\"book\" + 0.004*\"team\" + 0.004*\"narrator\" + 0.003*\"jeremy\"\n",
      "Topic #1: 0.010*\"book\" + 0.009*\"war\" + 0.006*\"in\" + 0.006*\"world\" + 0.005*\"novel\" + 0.005*\"states\" + 0.004*\"also\" + 0.004*\"new\" + 0.004*\"chapter\" + 0.004*\"story\"\n",
      "Topic #2: 0.008*\"he\" + 0.007*\"she\" + 0.006*\"mother\" + 0.006*\"one\" + 0.005*\"tells\" + 0.005*\"back\" + 0.005*\"house\" + 0.005*\"father\" + 0.005*\"school\" + 0.005*\"go\"\n",
      "Topic #3: 0.007*\"life\" + 0.006*\"love\" + 0.006*\"family\" + 0.006*\"father\" + 0.006*\"he\" + 0.006*\"novel\" + 0.005*\"young\" + 0.005*\"she\" + 0.004*\"story\" + 0.004*\"one\"\n",
      "Topic #4: 0.007*\"he\" + 0.006*\"one\" + 0.004*\"murder\" + 0.004*\"police\" + 0.004*\"man\" + 0.003*\"two\" + 0.003*\"case\" + 0.003*\"also\" + 0.003*\"would\" + 0.003*\"time\"\n",
      "Topic #5: 0.007*\"earth\" + 0.006*\"one\" + 0.005*\"time\" + 0.005*\"human\" + 0.005*\"world\" + 0.004*\"new\" + 0.004*\"planet\" + 0.004*\"life\" + 0.003*\"space\" + 0.003*\"he\"\n",
      "Topic #6: 0.006*\"he\" + 0.005*\"they\" + 0.005*\"one\" + 0.004*\"back\" + 0.004*\"find\" + 0.004*\"ship\" + 0.004*\"king\" + 0.003*\"two\" + 0.003*\"city\" + 0.003*\"help\"\n",
      "Topic #7: 0.006*\"alex\" + 0.005*\"one\" + 0.005*\"henry\" + 0.004*\"freddy\" + 0.004*\"time\" + 0.004*\"simon\" + 0.004*\"he\" + 0.004*\"luce\" + 0.004*\"new\" + 0.004*\"find\"\n",
      "Topic #8: 0.006*\"will\" + 0.006*\"jason\" + 0.005*\"he\" + 0.005*\"vampire\" + 0.005*\"king\" + 0.004*\"leo\" + 0.004*\"one\" + 0.004*\"new\" + 0.004*\"in\" + 0.003*\"world\"\n",
      "Topic #9: 0.005*\"jake\" + 0.005*\"charlie\" + 0.005*\"she\" + 0.004*\"he\" + 0.004*\"one\" + 0.004*\"roger\" + 0.004*\"back\" + 0.004*\"they\" + 0.004*\"luke\" + 0.004*\"new\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, model.print_topic(idx, 10))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.305*\"he\" + 0.215*\"one\" + 0.150*\"she\" + 0.140*\"time\" + 0.132*\"back\" + '\n",
      "  '0.131*\"also\" + 0.127*\"two\" + 0.125*\"they\" + 0.123*\"tells\" + 0.118*\"in\"'),\n",
      " (1,\n",
      "  '0.493*\"tom\" + 0.226*\"sophia\" + 0.182*\"mrs\" + 0.178*\"house\" + 0.161*\"she\" + '\n",
      "  '0.154*\"father\" + 0.147*\"mr\" + 0.146*\"he\" + 0.138*\"tells\" + -0.126*\"one\"'),\n",
      " (2,\n",
      "  '-0.558*\"tom\" + -0.252*\"sophia\" + 0.213*\"she\" + 0.190*\"he\" + -0.185*\"mrs\" + '\n",
      "  '0.163*\"tells\" + 0.144*\"mother\" + -0.138*\"mr\" + -0.129*\"western\" + '\n",
      "  '-0.102*\"however\"'),\n",
      " (3,\n",
      "  '-0.233*\"they\" + -0.203*\"ship\" + 0.187*\"he\" + -0.183*\"david\" + -0.178*\"back\" '\n",
      "  '+ -0.165*\"tells\" + 0.161*\"life\" + 0.160*\"family\" + 0.154*\"narrator\" + '\n",
      "  '-0.154*\"find\"'),\n",
      " (4,\n",
      "  '0.664*\"he\" + -0.258*\"mother\" + -0.213*\"she\" + -0.195*\"father\" + '\n",
      "  '-0.180*\"family\" + 0.121*\"narrator\" + 0.120*\"monk\" + -0.100*\"school\" + '\n",
      "  '-0.099*\"novel\" + -0.095*\"children\"'),\n",
      " (5,\n",
      "  '0.486*\"david\" + -0.241*\"king\" + 0.169*\"rosa\" + 0.162*\"book\" + '\n",
      "  '0.126*\"harlan\" + -0.120*\"he\" + 0.111*\"she\" + 0.108*\"gould\" + -0.108*\"anita\" '\n",
      "  '+ 0.103*\"would\"'),\n",
      " (6,\n",
      "  '-0.698*\"anita\" + -0.471*\"richard\" + 0.155*\"ship\" + 0.133*\"jacky\" + '\n",
      "  '-0.085*\"edward\" + -0.084*\"power\" + -0.078*\"monk\" + 0.073*\"father\" + '\n",
      "  '-0.071*\"scene\" + -0.070*\"kill\"'),\n",
      " (7,\n",
      "  '-0.397*\"david\" + -0.357*\"king\" + 0.221*\"jacky\" + 0.190*\"ship\" + '\n",
      "  '0.145*\"monk\" + 0.134*\"doctor\" + -0.130*\"rosa\" + -0.121*\"prince\" + '\n",
      "  '-0.118*\"arthur\" + -0.109*\"book\"'),\n",
      " (8,\n",
      "  '-0.288*\"harry\" + 0.283*\"she\" + -0.261*\"narrator\" + 0.228*\"jacky\" + '\n",
      "  '-0.224*\"david\" + -0.195*\"monk\" + -0.143*\"natalie\" + 0.130*\"ship\" + '\n",
      "  '0.123*\"says\" + 0.122*\"king\"'),\n",
      " (9,\n",
      "  '0.451*\"harry\" + -0.411*\"narrator\" + 0.261*\"monk\" + -0.221*\"david\" + '\n",
      "  '-0.215*\"anita\" + 0.182*\"natalie\" + -0.168*\"ship\" + -0.163*\"he\" + '\n",
      "  '-0.153*\"richard\" + 0.152*\"dresden\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsamodel = LsiModel(corpus, num_topics=10, id2word = id2word)  # train model\n",
    "\n",
    "pprint(lsamodel.print_topics(num_topics=10, num_words=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.305*\"he\" + 0.215*\"one\" + 0.150*\"she\" + 0.140*\"time\" + 0.132*\"back\" + 0.131*\"also\" + 0.127*\"two\" + 0.125*\"they\" + 0.123*\"tells\" + 0.118*\"in\"\n",
      "Topic #1: 0.493*\"tom\" + 0.226*\"sophia\" + 0.182*\"mrs\" + 0.178*\"house\" + 0.161*\"she\" + 0.154*\"father\" + 0.147*\"mr\" + 0.146*\"he\" + 0.138*\"tells\" + -0.126*\"one\"\n",
      "Topic #2: -0.558*\"tom\" + -0.252*\"sophia\" + 0.213*\"she\" + 0.190*\"he\" + -0.185*\"mrs\" + 0.163*\"tells\" + 0.144*\"mother\" + -0.138*\"mr\" + -0.129*\"western\" + -0.102*\"however\"\n",
      "Topic #3: -0.233*\"they\" + -0.203*\"ship\" + 0.187*\"he\" + -0.183*\"david\" + -0.178*\"back\" + -0.165*\"tells\" + 0.161*\"life\" + 0.160*\"family\" + 0.154*\"narrator\" + -0.154*\"find\"\n",
      "Topic #4: 0.664*\"he\" + -0.258*\"mother\" + -0.213*\"she\" + -0.195*\"father\" + -0.180*\"family\" + 0.121*\"narrator\" + 0.120*\"monk\" + -0.100*\"school\" + -0.099*\"novel\" + -0.095*\"children\"\n",
      "Topic #5: 0.486*\"david\" + -0.241*\"king\" + 0.169*\"rosa\" + 0.162*\"book\" + 0.126*\"harlan\" + -0.120*\"he\" + 0.111*\"she\" + 0.108*\"gould\" + -0.108*\"anita\" + 0.103*\"would\"\n",
      "Topic #6: -0.698*\"anita\" + -0.471*\"richard\" + 0.155*\"ship\" + 0.133*\"jacky\" + -0.085*\"edward\" + -0.084*\"power\" + -0.078*\"monk\" + 0.073*\"father\" + -0.071*\"scene\" + -0.070*\"kill\"\n",
      "Topic #7: -0.397*\"david\" + -0.357*\"king\" + 0.221*\"jacky\" + 0.190*\"ship\" + 0.145*\"monk\" + 0.134*\"doctor\" + -0.130*\"rosa\" + -0.121*\"prince\" + -0.118*\"arthur\" + -0.109*\"book\"\n",
      "Topic #8: -0.288*\"harry\" + 0.283*\"she\" + -0.261*\"narrator\" + 0.228*\"jacky\" + -0.224*\"david\" + -0.195*\"monk\" + -0.143*\"natalie\" + 0.130*\"ship\" + 0.123*\"says\" + 0.122*\"king\"\n",
      "Topic #9: 0.451*\"harry\" + -0.411*\"narrator\" + 0.261*\"monk\" + -0.221*\"david\" + -0.215*\"anita\" + 0.182*\"natalie\" + -0.168*\"ship\" + -0.163*\"he\" + -0.153*\"richard\" + 0.152*\"dresden\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, lsamodel.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
