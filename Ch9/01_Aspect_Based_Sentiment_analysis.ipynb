{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5rjUW4ONCy3"
      },
      "source": [
        "In this notebook we will deomonstrate aspect based sentiment analysis using [Varder](https://github.com/cjhutto/vaderSentiment) and [Stanford Core NLP](https://stanfordnlp.github.io/CoreNLP/index.html).<br>\n",
        "<br>**VADER Sentiment Analysis**: VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.(source:[github](https://github.com/cjhutto/vaderSentiment))<br>\n",
        "Stanford NLP have a live demo of aspect based sentiment analysis [here](http://nlp.stanford.edu:8080/sentiment/rntnDemo.html).<br><br>\n",
        "**Stanford Core NLP**: \"Most sentiment prediction systems work just by looking at words in isolation, giving positive points for positive words and negative points for negative words and then summing up these points. That way, the order of words is ignored and important information is lost. In constrast, our new deep learning model actually builds up a representation of whole sentences based on the sentence structure. It computes the sentiment based on how words compose the meaning of longer phrases. This way, the model is not as easily fooled as previous models.\"(source: [Stanford Core NLP](https://nlp.stanford.edu/sentiment/index.html).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tozj_pg3NCy5",
        "outputId": "ae1ce7e3-59b2-4118-d097-adfdef3084d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment==3.3.2\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/126.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m122.9/126.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment==3.3.2) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment==3.3.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment==3.3.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment==3.3.2) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment==3.3.2) (2023.7.22)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting pycorenlp==0.3.0\n",
            "  Downloading pycorenlp-0.3.0.tar.gz (1.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pycorenlp==0.3.0) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp==0.3.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp==0.3.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp==0.3.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp==0.3.0) (2023.7.22)\n",
            "Building wheels for collected packages: pycorenlp\n",
            "  Building wheel for pycorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycorenlp: filename=pycorenlp-0.3.0-py3-none-any.whl size=2120 sha256=5f7a1a04fb3099ae1c37fddc3a2fd2c98519f01fed3788cbb04587e75b90acbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/91/be/b83633256a1655afb34c5ea44b3290af84417a144e1f13e56f\n",
            "Successfully built pycorenlp\n",
            "Installing collected packages: pycorenlp\n",
            "Successfully installed pycorenlp-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install vaderSentiment==3.3.2\n",
        "!pip install pycorenlp==0.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzCuNsAkNCy6"
      },
      "source": [
        "### Importing the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6RRtyftNCy7",
        "outputId": "577de9a7-6cc7-4375-a537-363a03815b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x5eqsHeNCy7"
      },
      "source": [
        "Lets analyze these three sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1hx902XUNCy8"
      },
      "outputs": [],
      "source": [
        "positive = \"This fried chicken tastes very good. It is juicy and perfectly cooked.\"\n",
        "negative = \"This fried chicken tasted bad. It is dry and overcooked.\"\n",
        "ambiguous = \"Except the amazing fried chicken everything else at the restaurant tastes very bad.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-89LM6BNCy8"
      },
      "source": [
        "### VarderSentiment\n",
        "It scores from -1 to 1. -1 being negative and 1 being positive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B6u32HwUNCy8"
      },
      "outputs": [],
      "source": [
        "def sentiment_analyzer_scores(text):\n",
        "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "    score = sentiment_analyzer.polarity_scores(text)\n",
        "    pprint(text)\n",
        "    pprint(score)\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXuqqyY2NCy8",
        "outputId": "a6167f05-b21e-428b-a5eb-5b1ef113ef43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive:\n",
            "'This fried chicken tastes very good. It is juicy and perfectly cooked.'\n",
            "{'compound': 0.8122, 'neg': 0.0, 'neu': 0.575, 'pos': 0.425}\n",
            "------------------------------\n",
            "Negative:\n",
            "'This fried chicken tasted bad. It is dry and overcooked.'\n",
            "{'compound': -0.5423, 'neg': 0.28, 'neu': 0.72, 'pos': 0.0}\n",
            "------------------------------\n",
            "Ambiguous:\n",
            "('Except the amazing fried chicken everything else at the restaurant tastes '\n",
            " 'very bad.')\n",
            "{'compound': 0.0018, 'neg': 0.204, 'neu': 0.592, 'pos': 0.204}\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"Positive:\")\n",
        "sentiment_analyzer_scores(positive)\n",
        "\n",
        "print(\"Negative:\")\n",
        "sentiment_analyzer_scores(negative)\n",
        "\n",
        "print(\"Ambiguous:\")\n",
        "sentiment_analyzer_scores(ambiguous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRLVYVKSNCy8"
      },
      "source": [
        "As expected the sentiment analyzer performed well on the positive and negative case. When taking into consideration the ambiguous sentence, it calculated the compound sentiment to be close to 0, i.e, neutral.<br>\n",
        "But it seems to be a negative comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FY-paJh-NCy8"
      },
      "outputs": [],
      "source": [
        "def get_word_sentiment(text):\n",
        "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    tokenized_text = nltk.word_tokenize(text)\n",
        "\n",
        "    positive_words=[]\n",
        "    neutral_words=[]\n",
        "    negative_words=[]\n",
        "    for word in tokenized_text:\n",
        "        if (sentiment_analyzer.polarity_scores(word)['compound']) >= 0.1:\n",
        "            positive_words.append(word)\n",
        "        elif (sentiment_analyzer.polarity_scores(word)['compound']) <= -0.1:\n",
        "            negative_words.append(word)\n",
        "        else:\n",
        "            neutral_words.append(word)\n",
        "    print(text)\n",
        "    print('Positive:',positive_words)\n",
        "    print('Negative:',negative_words)\n",
        "    print('Neutral:',neutral_words)\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsuVdrxsNCy9",
        "outputId": "b8003b96-3d80-4159-b1eb-434e8c89346a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This fried chicken tastes very good. It is juicy and perfectly cooked.\n",
            "Positive: ['good', 'perfectly']\n",
            "Negative: []\n",
            "Neutral: ['This', 'fried', 'chicken', 'tastes', 'very', '.', 'It', 'is', 'juicy', 'and', 'cooked', '.']\n",
            "------------------------------\n",
            "This fried chicken tasted bad. It is dry and overcooked.\n",
            "Positive: []\n",
            "Negative: ['bad']\n",
            "Neutral: ['This', 'fried', 'chicken', 'tasted', '.', 'It', 'is', 'dry', 'and', 'overcooked', '.']\n",
            "------------------------------\n",
            "Except the amazing fried chicken everything else at the restaurant tastes very bad.\n",
            "Positive: ['amazing']\n",
            "Negative: ['bad']\n",
            "Neutral: ['Except', 'the', 'fried', 'chicken', 'everything', 'else', 'at', 'the', 'restaurant', 'tastes', 'very', '.']\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "get_word_sentiment(positive)\n",
        "get_word_sentiment(negative)\n",
        "get_word_sentiment(ambiguous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P4vmM0mNCy9"
      },
      "source": [
        "### Stanford Core NLP\n",
        "Before moving on to execute the code we need to start the Stanford Core NLP server on our local machine.<br> To do that follow the steps below (tested on debian should work fine for other distributions too):\n",
        "1. Download the Stanford Core NLP model from [here](https://stanfordnlp.github.io/CoreNLP/#download).\n",
        "2. Unizip the folder\n",
        "3. cd into the folder<br>\n",
        "    ```cd stanford-corenlp-4.0.0/```\n",
        "4. Start the server using this command:<br>\n",
        "    ```java -mx5g -cp \"./*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000```\n",
        "<br><br>\n",
        "If you do not have java installed on your system please install it from the official [Oracle](https://www.oracle.com/in/java/technologies/javase-downloads.html) page.\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stanford Core NLP\n",
        "## Same can be performed in colab using the below command\n",
        "1. Download the Stanford Core NLP model\n",
        "```\n",
        "wget 'https://nlp.stanford.edu/software/stanford-corenlp-4.5.4.zip'\n",
        "```\n",
        "Note: The version may differ thus you may need to use above link to download latest version\n",
        "\n",
        "2. Unzip the File\n",
        "```\n",
        "unzip stanford-corenlp-4.5.4.zip\n",
        "```\n",
        "\n",
        "3. Move to the Directory\n",
        "```\n",
        "cd stanford-corenlp-4.5.4\n",
        "```\n",
        "\n",
        "4. Start the server using this command:\n",
        "```\n",
        "java -mx5g -cp \"./*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000\n",
        "```\n",
        "\n",
        "## Or Just Run the Below Cell and Change the version as Required."
      ],
      "metadata": {
        "id": "hWhaJWZROGm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "version='4.5.4'\n",
        "wget https://nlp.stanford.edu/software/stanford-corenlp-${version}.zip\n",
        "unzip stanford-corenlp-${version}.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qt-LaYpNnTi",
        "outputId": "e33cab7d-efa1-4565-fa43-bd4456ceb7e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-04 13:24:15--  https://nlp.stanford.edu/software/stanford-corenlp-4.5.4.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.4.zip [following]\n",
            "--2023-09-04 13:24:16--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.4.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 506470124 (483M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-4.5.4.zip’\n",
            "\n",
            "stanford-corenlp-4. 100%[===================>] 483.01M  5.13MB/s    in 92s     \n",
            "\n",
            "2023-09-04 13:25:48 (5.27 MB/s) - ‘stanford-corenlp-4.5.4.zip’ saved [506470124/506470124]\n",
            "\n",
            "Archive:  stanford-corenlp-4.5.4.zip\n",
            "   creating: stanford-corenlp-4.5.4/\n",
            "  inflating: stanford-corenlp-4.5.4/Makefile  \n",
            "  inflating: stanford-corenlp-4.5.4/joda-time-2.10.5-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/ejml-ddense-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/ejml-simple-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/corenlp.sh  \n",
            "  inflating: stanford-corenlp-4.5.4/LICENSE.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/istack-commons-runtime-3.0.7-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/ejml-core-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/README.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/sample-project-pom.xml  \n",
            "  inflating: stanford-corenlp-4.5.4/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/ejml-core-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/LIBRARY-LICENSES  \n",
            "  inflating: stanford-corenlp-4.5.4/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-4.5.4/stanford-corenlp-4.5.4-javadoc.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-4.5.4/input.txt.out  \n",
            "  inflating: stanford-corenlp-4.5.4/pom.xml  \n",
            "  inflating: stanford-corenlp-4.5.4/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/xom.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/input.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-4.5.4/ejml-simple-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/stanford-corenlp-4.5.4-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/jollyday.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/input.txt.xml  \n",
            "   creating: stanford-corenlp-4.5.4/sutime/\n",
            "  inflating: stanford-corenlp-4.5.4/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/sutime/english.holidays.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-4.5.4/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/stanford-corenlp-4.5.4-models.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/StanfordDependenciesManual.pdf  \n",
            "  inflating: stanford-corenlp-4.5.4/stanford-corenlp-4.5.4.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/build.xml  \n",
            "  inflating: stanford-corenlp-4.5.4/javax.activation-api-1.2.0-sources.jar  \n",
            "   creating: stanford-corenlp-4.5.4/patterns/\n",
            "  inflating: stanford-corenlp-4.5.4/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/patterns/names.txt  \n",
            " extracting: stanford-corenlp-4.5.4/patterns/places.txt  \n",
            " extracting: stanford-corenlp-4.5.4/patterns/goldplaces.txt  \n",
            " extracting: stanford-corenlp-4.5.4/patterns/otherpeople.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/patterns/example.properties  \n",
            "  inflating: stanford-corenlp-4.5.4/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-4.5.4/ejml-ddense-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/pom-java-17.xml  \n",
            "   creating: stanford-corenlp-4.5.4/tokensregex/\n",
            "  inflating: stanford-corenlp-4.5.4/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-4.5.4/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-4.5.4/joda-time.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/xom-1.3.8-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/istack-commons-runtime-3.0.7.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/protobuf-java-3.19.6.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.4/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-4.5.4/javax.json.jar  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd stanford-corenlp-4.5.4/\n",
        "nohup java -mx5g -cp './*' edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000 --port 9001 > corenlp.log 2>&1 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l0suKUzSBfe",
        "outputId": "9801cd0a-3f2e-45e0-f2a1-0566ec0e1df5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep java\n",
        "!killall java\n",
        "!ps aux | grep java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9iBlQ5jYBt4",
        "outputId": "e3c599ab-233c-4e7f-ac93-27535fb2c212"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        2027  0.0  0.0   7372  3548 ?        S    13:29   0:00 /bin/bash -c ps aux | grep java\n",
            "root        2029  0.0  0.0   6480  2400 ?        S    13:29   0:00 grep java\n",
            "java: no process found\n",
            "root        2031  0.0  0.0   7372  3468 ?        S    13:29   0:00 /bin/bash -c ps aux | grep java\n",
            "root        2033  0.0  0.0   6480  2372 ?        S    13:29   0:00 grep java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "l3xzcuPUNCy9"
      },
      "outputs": [],
      "source": [
        "nlp = StanfordCoreNLP('http://localhost:9001')\n",
        "\n",
        "def get_sentiment(text):\n",
        "    res = json.loads(nlp.annotate(text,\n",
        "                       properties={'annotators': 'sentiment',\n",
        "                                   'outputFormat': 'json',\n",
        "                                   'timeout': 10000,\n",
        "                       }))\n",
        "    print(text)\n",
        "    print('Sentiment:', res['sentences'][0]['sentiment'])\n",
        "    print('Sentiment score:', res['sentences'][0]['sentimentValue'])\n",
        "    print('Sentiment distribution (0-v. negative, 5-v. positive:', res['sentences'][0]['sentimentDistribution'])\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9P77HpcNCy9",
        "outputId": "2efaad5c-f20c-43fc-c1f6-db2e3a57aa20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This fried chicken tastes very good. It is juicy and perfectly cooked.\n",
            "Sentiment: Negative\n",
            "Sentiment score: 1\n",
            "Sentiment distribution (0-v. negative, 5-v. positive: [0.12830923698552, 0.37878858949882, 0.30518256344905, 0.17180670417797, 0.01591290588864]\n",
            "------------------------------\n",
            "This fried chicken tasted bad. It is dry and overcooked.\n",
            "Sentiment: Negative\n",
            "Sentiment score: 1\n",
            "Sentiment distribution (0-v. negative, 5-v. positive: [0.35691292388455, 0.38793571113551, 0.18201904294799, 0.04194609175503, 0.03118623027692]\n",
            "------------------------------\n",
            "Except the amazing fried chicken everything else at the restaurant tastes very bad.\n",
            "Sentiment: Negative\n",
            "Sentiment score: 1\n",
            "Sentiment distribution (0-v. negative, 5-v. positive: [0.12830923590495, 0.37878858881094, 0.30518256399302, 0.1718067054989, 0.01591290579219]\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "get_sentiment(positive)\n",
        "get_sentiment(negative)\n",
        "get_sentiment(ambiguous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoC4-2lLNCy9"
      },
      "source": [
        "Here you see the model successfully predicts the ambigous sentence which the Varder failed to predict correctly.<br>\n",
        "The code in this notebook has been adapted from this [article](https://towardsdatascience.com/sentiment-analysis-beyond-words-6ca17a6c1b54).See below code for colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mKdifBJ5NNNu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}